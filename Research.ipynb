{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "print(df.head(20))"
      ],
      "metadata": {
        "id": "-PiTRN2FP495"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Split features and labels\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize SVM classifier\n",
        "clf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "7sG75luNQxg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Split features and labels\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model Selection and Parameter Tuning\n",
        "models = [\n",
        "    {\n",
        "        'name': 'SVM',\n",
        "        'estimator': SVC(),\n",
        "        'params': {\n",
        "            'kernel': ['linear', 'rbf', 'poly'],\n",
        "            'C': [0.1, 1, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'estimator': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 5, 10]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    grid_search = GridSearchCV(model['estimator'], model['params'], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_pred = best_estimator.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{model['name']} Accuracy: {accuracy}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\\n\")\n"
      ],
      "metadata": {
        "id": "3E9YOpuBP75b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Results\n",
        "models_results = {\n",
        "    'SVM': {'accuracy': 0.508, 'best_params': {'C': 10, 'kernel': 'poly'}},\n",
        "    'Random Forest': {'accuracy': 0.590, 'best_params': {'max_depth': 5, 'n_estimators': 50}}\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Accuracy bar plot\n",
        "ax.bar(models_results.keys(), [result['accuracy'] for result in models_results.values()], color=['blue', 'green'])\n",
        "\n",
        "# Adding accuracy values on top of bars\n",
        "for i, (model, result) in enumerate(models_results.items()):\n",
        "    ax.text(i, result['accuracy'] + 0.01, f\"{result['accuracy']:.3f}\", ha='center', fontsize=12)\n",
        "\n",
        "# Adding best parameters as annotations\n",
        "for i, (model, result) in enumerate(models_results.items()):\n",
        "    best_params = \", \".join([f\"{k}: {v}\" for k, v in result['best_params'].items()])\n",
        "    ax.text(i, 0.5, f\"Best Params:\\n{best_params}\", ha='center', va='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "# Title and labels\n",
        "ax.set_title('Comparison of SVM and Random Forest Classifiers', fontsize=16)\n",
        "ax.set_ylabel('Accuracy', fontsize=14)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_uLSAtVoRgcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np  # Add this import statement\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Data distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=data, x='TP9', hue='label', kde=True, bins=20)\n",
        "plt.title('Distribution of TP9 feature by Label')\n",
        "plt.xlabel('TP9')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(title='Label')\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance with Random Forest\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "sns.barplot(x=importances[indices], y=X.columns[indices])\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Decision boundaries using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
        "plt.title('Decision Boundaries (PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rLFl4NFbSMjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Data distribution for all electrodes\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, electrode in enumerate(['TP9', 'AF7', 'AF8', 'TP10'], start=1):\n",
        "    plt.subplot(2, 2, i)\n",
        "    sns.histplot(data=data, x=electrode, hue='label', kde=True, bins=20)\n",
        "    plt.title(f'Distribution of {electrode} feature by Label')\n",
        "    plt.xlabel(electrode)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend(title='Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance with Random Forest\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "importances = rf.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "sns.barplot(x=importances[indices], y=X.columns[indices])\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "# Decision boundaries using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
        "plt.title('Decision Boundaries (PCA)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CmhAznsETuJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Split features and labels\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for test set\n",
        "y_pred_gbc = gbc.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_gbc = accuracy_score(y_test, y_pred_gbc)\n",
        "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_gbc)\n",
        "\n",
        "# Plot decision boundaries using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis')\n",
        "plt.title('Decision Boundaries (PCA) - Gradient Boosting Classifier')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zTz_lmcnT-td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have the dataset loaded into X and y\n",
        "# For example:\n",
        "# X = your feature data\n",
        "# y = your labels (0 for no movement, 1 for movement)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Selection and Parameter Tuning\n",
        "models = [\n",
        "    {\n",
        "        'name': 'SVM',\n",
        "        'estimator': SVC(),\n",
        "        'params': {\n",
        "            'kernel': ['linear', 'rbf', 'poly'],\n",
        "            'C': [0.1, 1, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'estimator': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Gradient Boosting',\n",
        "        'estimator': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.5]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Logistic Regression',\n",
        "        'estimator': LogisticRegression(),\n",
        "        'params': {\n",
        "            'C': [0.1, 1, 10],\n",
        "            'solver': ['liblinear', 'lbfgs']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'K-Nearest Neighbors',\n",
        "        'estimator': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'weights': ['uniform', 'distance']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    grid_search = GridSearchCV(model['estimator'], model['params'], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_pred = best_estimator.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{model['name']} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\\n\")\n"
      ],
      "metadata": {
        "id": "qIX-VWIDDbXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming you have the dataset loaded into X and y\n",
        "# For example:\n",
        "# X = your feature data\n",
        "# y = your labels (0 for no movement, 1 for movement)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Selection and Parameter Tuning\n",
        "models = [\n",
        "    {\n",
        "        'name': 'SVM',\n",
        "        'estimator': SVC(),\n",
        "        'params': {\n",
        "            'kernel': ['linear', 'rbf', 'poly'],\n",
        "            'C': [0.01, 0.1, 1, 10, 100]  # Adjusting C for SVM regularization\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'estimator': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],  # Trying more estimators\n",
        "            'max_depth': [None, 5, 10, 20]  # Adjusting max depth\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Gradient Boosting',\n",
        "        'estimator': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],  # Trying more estimators\n",
        "            'learning_rate': [0.01, 0.1, 0.5],  # Adjusting learning rate\n",
        "            'max_depth': [3, 5, 10]  # Adjusting max depth\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Logistic Regression',\n",
        "        'estimator': LogisticRegression(),\n",
        "        'params': {\n",
        "            'C': [0.01, 0.1, 1, 10, 100],  # Adjusting regularization strength\n",
        "            'solver': ['liblinear', 'lbfgs']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'K-Nearest Neighbors',\n",
        "        'estimator': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7, 9],  # Trying more neighbors\n",
        "            'weights': ['uniform', 'distance']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    grid_search = GridSearchCV(model['estimator'], model['params'], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_pred = best_estimator.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{model['name']} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\\n\")\n"
      ],
      "metadata": {
        "id": "QpFxBOQ3WiSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('ArmMovementDetection_Dataset.csv')\n",
        "\n",
        "# Assuming your dataset has a 'label' column and feature columns start from column 1 to end\n",
        "# Modify if your dataset structure is different\n",
        "\n",
        "# Check the number of samples in each class before balancing\n",
        "print(\"Number of samples in each class before balancing:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Balance the dataset by downsampling the majority class\n",
        "min_class = df['label'].value_counts().idxmin()\n",
        "df_balanced = df.groupby('label').apply(lambda x: x.sample(n=df['label'].value_counts()[min_class])).reset_index(drop=True)\n",
        "\n",
        "# Check the number of samples in each class after balancing\n",
        "print(\"Number of samples in each class after balancing:\")\n",
        "print(df_balanced['label'].value_counts())\n",
        "\n",
        "X_balanced = df_balanced.iloc[:, 1:].values\n",
        "y_balanced = df_balanced.iloc[:, 0].values\n",
        "\n",
        "# Split the data into training and validation sets (70% training, 30% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Combine the training and validation labels\n",
        "y_combined = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "# Fit the LabelEncoder on the combined labels\n",
        "le = LabelEncoder()\n",
        "y_combined_encoded = le.fit_transform(y_combined)\n",
        "\n",
        "# Transform the training and validation labels\n",
        "y_train_encoded = y_combined_encoded[:len(y_train)]\n",
        "y_val_encoded = y_combined_encoded[len(y_train):]\n",
        "\n",
        "# Count the number of unique classes in the combined labels\n",
        "num_classes = len(np.unique(y_combined_encoded))\n",
        "\n",
        "# Build the neural network model with the correct number of output units\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Adjust according to the number of unique classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_encoded, epochs=50, validation_data=(X_val, y_val_encoded), batch_size=32)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = np.argmax(model.predict(X_val), axis=1)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_val_encoded, y_pred)\n",
        "precision = precision_score(y_val_encoded, y_pred, average='weighted')\n",
        "recall = recall_score(y_val_encoded, y_pred, average='weighted')\n",
        "f1 = f1_score(y_val_encoded, y_pred, average='weighted')\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_val_encoded, y_pred)\n",
        "\n",
        "# True Positives, True Negatives, False Positives, False Negatives\n",
        "tp = cm[1, 1]\n",
        "tn = cm[0, 0]\n",
        "fp = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "\n",
        "# Plotting training and validation accuracy over epochs\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n",
        "\n",
        "# Save the model if needed\n",
        "model.save('eeg_movement_detection_model.h5')\n"
      ],
      "metadata": {
        "id": "yoFoQA-kmaKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_original = data['label']\n",
        "class_counts_before_balancing = y_original.value_counts()\n",
        "\n",
        "print(\"Number of samples in each class before balancing:\")\n",
        "print(class_counts_before_balancing)"
      ],
      "metadata": {
        "id": "8uRP11hzcYuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_balanced, y_balanced = oversampler.fit_resample(X, y)\n",
        "\n",
        "print(\"Number of samples in each class after balancing:\")\n",
        "print(y_balanced.value_counts())"
      ],
      "metadata": {
        "id": "85PMlqVdcdwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Display the head of the dataset\n",
        "print(\"Head of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Oversampling to balance the classes\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_balanced, y_balanced = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Check the number of samples in each class after balancing\n",
        "print(\"Number of samples in each class after balancing:\")\n",
        "print(y_balanced.value_counts())\n",
        "\n",
        "# Split the data into training and validation sets (70% training, 30% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on the validation set\n",
        "y_pred = rf_classifier.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "id": "l9ibe83w1-4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"ArmMovementDetection_Dataset.csv\")\n",
        "\n",
        "# Display the head of the dataset\n",
        "print(\"Head of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop(columns=['label'])\n",
        "y = data['label']\n",
        "\n",
        "# Oversampling to balance the classes\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_balanced, y_balanced = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Check the number of samples in each class after balancing\n",
        "print(\"Number of samples in each class after balancing:\")\n",
        "print(y_balanced.value_counts())\n",
        "\n",
        "# Split the data into training and validation sets (70% training, 30% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on the validation set\n",
        "y_pred = rf_classifier.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# Model Selection and Parameter Tuning\n",
        "models = [\n",
        "    {\n",
        "        'name': 'SVM',\n",
        "        'estimator': SVC(),\n",
        "        'params': {\n",
        "            'kernel': ['linear', 'rbf', 'poly'],\n",
        "            'C': [0.01, 0.1, 1, 10, 100]  # Adjusting C for SVM regularization\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'estimator': RandomForestClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],  # Trying more estimators\n",
        "            'max_depth': [None, 5, 10, 20]  # Adjusting max depth\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Gradient Boosting',\n",
        "        'estimator': GradientBoostingClassifier(),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200, 300],  # Trying more estimators\n",
        "            'learning_rate': [0.01, 0.1, 0.5],  # Adjusting learning rate\n",
        "            'max_depth': [3, 5, 10]  # Adjusting max depth\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Logistic Regression',\n",
        "        'estimator': LogisticRegression(),\n",
        "        'params': {\n",
        "            'C': [0.01, 0.1, 1, 10, 100],  # Adjusting regularization strength\n",
        "            'solver': ['liblinear', 'lbfgs']\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'K-Nearest Neighbors',\n",
        "        'estimator': KNeighborsClassifier(),\n",
        "        'params': {\n",
        "            'n_neighbors': [3, 5, 7, 9],  # Trying more neighbors\n",
        "            'weights': ['uniform', 'distance']\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    grid_search = GridSearchCV(model['estimator'], model['params'], cv=5, scoring='accuracy')\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_estimator = grid_search.best_estimator_\n",
        "    y_pred = best_estimator.predict(X_val)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "\n",
        "    print(f\"{model['name']} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\")\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\\n\")\n"
      ],
      "metadata": {
        "id": "sFtioqLZ6Zd2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}